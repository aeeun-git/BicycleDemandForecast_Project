import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Attention, Input, Concatenate
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import urllib.parse
import requests
import datetime
from bs4 import BeautifulSoup

# âœ… 1ï¸âƒ£ ê³µíœ´ì¼ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
def get_holiday(year: int) -> pd.DataFrame:
    url = "http://apis.data.go.kr/B090041/openapi/service/SpcdeInfoService/getRestDeInfo"
    api_key_utf8 = "%2F4sT8Q69IaqXlijmshQntrOY0PFzZqnWgtvPQv4p1i8rMTD6k4Gn9MxteMgy1W%2Bo%2Fl2N3GLS7f1hqGmpo%2F3QNQ%3D%3D"  # ê°œì¸ API í‚¤ ì…ë ¥
    api_key_decode = urllib.parse.unquote(api_key_utf8)

    temp = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]
    item_list = []

    for month in range(1, 7):  # 1~6ì›”ë§Œ ê°€ì ¸ì˜¤ê¸°
        params = {"ServiceKey": api_key_decode, "solYear": year, "solMonth": str(month).zfill(2), "numOfRows": 100}
        response = requests.get(url, params=params)

        if response.status_code != 200:
            print(f"ğŸ“Œ {year}ë…„ {month}ì›” API ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
            continue

        xml = BeautifulSoup(response.text, "html.parser")
        items = xml.find_all("item")

        for item in items:
            dt = datetime.datetime.strptime(item.find("locdate").text.strip(), "%Y%m%d")
            item_dict = {"ê³µíœ´ì¼ëª…": item.find("datename").text.strip(), "ë‚ ì§œ": dt.date(), "ìš”ì¼": temp[dt.weekday()]}
            item_list.append(item_dict)

    return pd.DataFrame(item_list)

holidays_2024 = get_holiday(2024)
holidays_2024 = set(holidays_2024["ë‚ ì§œ"])  # ë¹ ë¥¸ ê²€ìƒ‰ì„ ìœ„í•´ set ë³€í™˜

# âœ… 2ï¸âƒ£ íŒŒì¼ ê²½ë¡œ ì„¤ì •
file_path_2401 = 'ì„œìš¸íŠ¹ë³„ì‹œ ê³µê³µìì „ê±° ëŒ€ì—¬ì´ë ¥ ì •ë³´_2401.csv'
file_path_2402 = 'ì„œìš¸íŠ¹ë³„ì‹œ ê³µê³µìì „ê±° ëŒ€ì—¬ì´ë ¥ ì •ë³´_2402.csv'
file_path_2403 = 'ì„œìš¸íŠ¹ë³„ì‹œ ê³µê³µìì „ê±° ëŒ€ì—¬ì´ë ¥ ì •ë³´_2403.csv'
file_path_2404 = 'ì„œìš¸íŠ¹ë³„ì‹œ ê³µê³µìì „ê±° ëŒ€ì—¬ì´ë ¥ ì •ë³´_2404.csv'
file_path_2405 = 'ì„œìš¸íŠ¹ë³„ì‹œ ê³µê³µìì „ê±° ëŒ€ì—¬ì´ë ¥ ì •ë³´_2405.csv'
file_path_2406 = 'ì„œìš¸íŠ¹ë³„ì‹œ ê³µê³µìì „ê±° ëŒ€ì—¬ì´ë ¥ ì •ë³´_2406.csv'
weather_path = 'ì„œìš¸íŠ¹ë³„ì‹œ 2024 01_06 ë‚ ì”¨ì •ë³´.csv'

# âœ… 3ï¸âƒ£ ë°ì´í„° ë¡œë“œ ë° ë³‘í•©
data_2401 = pd.read_csv(file_path_2401, encoding='cp949')
data_2402 = pd.read_csv(file_path_2402, encoding='cp949')
data_2403 = pd.read_csv(file_path_2403, encoding='cp949')
data_2404 = pd.read_csv(file_path_2404, encoding='cp949')
data_2405 = pd.read_csv(file_path_2405, encoding='cp949')
data_2406 = pd.read_csv(file_path_2406, encoding='cp949')

data = pd.concat([data_2401, data_2402, data_2403, data_2404, data_2405, data_2406], ignore_index=True)
data.rename(columns={'ëŒ€ì—¬ ëŒ€ì—¬ì†Œëª…': 'RENT_NM', 'ëŒ€ì—¬ì¼ì‹œ': 'RENT_DT'}, inplace=True)

# âœ… 4ï¸âƒ£ íŠ¹ì • ëŒ€ì—¬ì†Œ ì„ íƒ (ì—¬ì˜ë‚˜ë£¨ì—­ 1ë²ˆì¶œêµ¬ ì•)
yeouinaru_data = data[data['RENT_NM'].str.contains('ì—¬ì˜ë‚˜ë£¨ì—­ 1ë²ˆì¶œêµ¬ ì•', na=False)]
yeouinaru_data['RENT_DT'] = pd.to_datetime(yeouinaru_data['RENT_DT'], errors='coerce')
yeouinaru_data['hour'] = yeouinaru_data['RENT_DT'].dt.hour
yeouinaru_data['date'] = yeouinaru_data['RENT_DT'].dt.date
hourly_demand = yeouinaru_data.groupby(['date', 'hour']).size().reset_index(name='demand')

# âœ… 5ï¸âƒ£ ë‚ ì”¨ ë°ì´í„° ë¡œë“œ ë° ë³‘í•©
weather_data = pd.read_csv(weather_path, encoding='cp949')
weather_data['ì¼ì‹œ'] = pd.to_datetime(weather_data['ì¼ì‹œ'])
weather_data['hour'] = weather_data['ì¼ì‹œ'].dt.hour
weather_data['date'] = weather_data['ì¼ì‹œ'].dt.date
weather_data = weather_data.drop(columns=['ì§€ì ', 'ì§€ì ëª…'])

# âœ… 6ï¸âƒ£ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ê°•ìˆ˜ëŸ‰, ì ì„¤ 0ìœ¼ë¡œ ì±„ìš°ê¸°)
weather_data['ê°•ìˆ˜ëŸ‰(mm)'] = weather_data['ê°•ìˆ˜ëŸ‰(mm)'].fillna(0)
weather_data['ì ì„¤(cm)'] = weather_data['ì ì„¤(cm)'].fillna(0)

# âœ… 7ï¸âƒ£ ë°ì´í„° ë³‘í•©
combined_data = pd.merge(hourly_demand, weather_data, on=['date', 'hour'], how='left')
combined_data = combined_data.fillna(0)
combined_data['weekday'] = pd.to_datetime(combined_data['date']).dt.weekday

# âœ… 8ï¸âƒ£ ê³µíœ´ì¼ ì •ë³´ ì¶”ê°€
combined_data['is_holiday'] = combined_data['date'].isin(holidays_2024).astype(int)
combined_data['before_holiday'] = (combined_data['date'] - pd.Timedelta(days=1)).isin(holidays_2024).astype(int)

# âœ… 9ï¸âƒ£ ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()

X_scaled = scaler_X.fit_transform(combined_data[['ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'ì ì„¤(cm)', 'í’ì†(m/s)', 'weekday', 'hour', 'is_holiday', 'before_holiday']])
y_scaled = scaler_y.fit_transform(combined_data['demand'].values.reshape(-1, 1))

# âœ… 1ï¸âƒ£0ï¸âƒ£ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
window_size = 6

def create_sequences(data, target, window_size):
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data[i:i + window_size])
        y.append(target[i + window_size])
    return np.array(X), np.array(y)

X, y = create_sequences(X_scaled, y_scaled, window_size)

# âœ… 1ï¸âƒ£1ï¸âƒ£ í•™ìŠµ ë°ì´í„° ë¶„í• 
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# âœ… 1ï¸âƒ£2ï¸âƒ£ ëª¨ë¸ êµ¬ì¶• (LSTM > Dropout > Attention > Concatenate > Dense)
input_layer = Input(shape=(X_train.shape[1], X_train.shape[2]))
lstm_out = LSTM(128, return_sequences=True)(input_layer)
dropout = Dropout(0.3)(lstm_out)
attention = Attention()([dropout, dropout])
concat = Concatenate()([dropout, attention])
dense_output = Dense(1)(concat)

# âœ… 1ï¸âƒ£3ï¸âƒ£ ëª¨ë¸ ì»´íŒŒì¼ ë° í•™ìŠµ
model = Model(inputs=input_layer, outputs=dense_output)
model.compile(optimizer='adam', loss='mean_squared_error')

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(X_train, y_train, epochs=30, batch_size=4, validation_data=(X_test, y_test),
                    callbacks=[early_stopping], verbose=1)

# âœ… 1ï¸âƒ£4ï¸âƒ£ ì˜ˆì¸¡ ë° í‰ê°€
y_pred = model.predict(X_test)
y_test_rescaled = scaler_y.inverse_transform(y_test)
y_pred_rescaled = np.maximum(scaler_y.inverse_transform(y_pred), 0)

print(f"RMSE: {np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))}, MAE: {mean_absolute_error(y_test_rescaled, y_pred_rescaled)}")
